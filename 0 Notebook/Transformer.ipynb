{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgYQypoMViiH"
      },
      "source": [
        "# Attention is All You Need\n",
        "\n",
        "Refer to paper [```Attention is all you need```](https://arxiv.org/abs/1706.03762)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F0Cc1dDET1nO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MEWU-dJlVIBT"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bq_9iO-Vsry"
      },
      "source": [
        "## Input Embedding\n",
        "* refers to the numerical representation of words. Say embedding dimension is 4, then each word will be represented in 4 numbers.\n",
        "\n",
        "example:\n",
        "* King:     ```-0.17``` ```-0.001``` ```-0.77``` ```0.91```\n",
        "* Queen:     ```-0.28``` ```-0.011``` ```-0.88``` ```0.74```\n",
        "* Mountain:  ```0.67``` ```0.691``` ```-0.007``` ```0.17```\n",
        "\n",
        "The word embedding are created in this format. They are represented in random values of given dimension that machine will understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TN88UGsaUEjG"
      },
      "outputs": [],
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model # dimension of embedding (the more the size, more it can distinguish each word)\n",
        "    self.vocab_size = vocab_size # total length/size of vocabulary\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model) # creates dictionary of word embeddings of size (vocab_size * dimension of embedding of model)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M82jhTFJZzO4"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "* In RNN and sequential models, each word of a sentence is used sequentially so the predictive model learns to act in that way. But in transformer, that is not the case. To denote position of words of input sentence, positional encoding is used.\n",
        "* It too is of a dimension (say 'd') and is added with input embedding and used in further tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PTBFAlmqVnIi"
      },
      "outputs": [],
      "source": [
        "class Positional_Embedding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, seq_length: int, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model # dimension of positional embedding\n",
        "    self.seq_length = seq_length # max length that a sentence can be\n",
        "    self.dropout = nn.Dropout(dropout) # dropout so that it won't overfit\n",
        "\n",
        "    # create a position vector of shape (seq_length,  1)\n",
        "    position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
        "    # Denominator in formula (using log for better stability and convergence)\n",
        "    div_term = torch.exp(torch.arange(0,d_model,2).float() * (- math.log(10000.0)/ d_model))\n",
        "\n",
        "    # Embedding matrix of shape (seq_length,  d_model)\n",
        "    pe = torch.zeros(seq_length, d_model)\n",
        "\n",
        "    # Apply sin to even positions and cosine to odd\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # since there will be batches, adding another dimension in the embedding\n",
        "    pe = pe.unsqueeze(0) # shape = (1, seq_length, d_model)\n",
        "\n",
        "\n",
        "    # to save the embedding tensor along in the file when saving state of the model\n",
        "    self.register_buffer(\"pe\", pe)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.pe[:, :x.shape[1], :]).requires_grad(False) #add positional embedding along with each data instance and Grad=False to disable learning embedding\n",
        "    return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PxTSvNnLtpw"
      },
      "source": [
        "## Layer Normalization\n",
        "\n",
        "* Say we have a batch of 3 items/words. Each are represented in form of tennsors having different means and variance. We use layer normalization i.e. normalization in only the layer of computation to get a new mean and variance for the layer.\n",
        "* Sometimes it may be needed to be modified so we also introduce two parameters gamma (multiplicative) and beta (additive).\n",
        "* Also we add epsilon in denominator to avoid divide by zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Lla92-tvLInc"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "  def __init__(self, eps: float = 10**-6) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.eps = eps\n",
        "    self.alpha = nn.Parameter(torch.ones(1)) # parameter is already learnable # multplicative\n",
        "    self.bias = nn.Parameter(torch.zeros(1)) # additive\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True) # get mean of dimension excluding batch. Retain the data of the dimension\n",
        "    std = x.std(dim=-1, keepdim=True)\n",
        "    return self.aplha * ((x-mean)/(std + self.eps)) + self.bias\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvnEaVBXBcPf"
      },
      "source": [
        "## Feed Forward Layer\n",
        "\n",
        "```FNN(x) = max(0,xW1+b1)W2+b2```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9WfMpEjaP9QU"
      },
      "outputs": [],
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model:int, d_ff:int, dropout:float) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
        "\n",
        "  def forward(self, x):\n",
        "    # we have batch of sentences of dimension (Batch, seq_len, d_model) converted to (Batch, seq_len, d_ff) and again into (Batch, seq_len, d_model)\n",
        "    return self.linear(self.dropout(torch.relu(self.linear1(x))))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90R-l7C2EBHF"
      },
      "source": [
        "## **MULTI-HEAD ATTENTION**\n",
        "\n",
        "* Multihead attention is the part in transformer that gives it an edge over other previously existing sequential model architectures for language.\n",
        "* Multi head attention is used to calculate attention scores for all the words in the sentence or text that helps the model to understand which part is important and focus more on that part.\n",
        "* Multi head attention can be calculated in parallel that makes it fast.\n",
        "* The embeddings/ tensors are duplicated or used 3 times as query, key and value to calculate attention scores.\n",
        "* Query: What to look for. **Input * W_q**\n",
        "* Key : What can I offer. **Input * W_k**\n",
        "* Value: What I actually offer. **Input * W_v**\n",
        "* h = heads (d_model should be exactly divisible by h)\n",
        "* d_k = d_v = d_model/h = dimension of key/value scores per head\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SsOiqMHQD2nk"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model:int, h:int, dropout:float):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    assert d_model % h ==0, \"Dimension of model is not exactly divisible by heads(h)\"\n",
        "\n",
        "    self.d_k = self.d_v = d_model // h\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model) # weight matrix of query\n",
        "    self.w_k = nn.Linear(d_model, d_model) # weight matrix of key\n",
        "    self.w_v = nn.Linear(d_model, d_model) # weight matrix of value\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model) # weight matrix of output\n",
        "\n",
        "\n",
        "\n",
        "# Create a function for attention calculation\n",
        "  @staticmethod # means that the function doesn't need an instance of MultiHeadAttention to be called\n",
        "  def attention_calc(query, key, value, mask, dropout:nn.Dropout()):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    attention_scores = (query @ key.transpose(2,3)) / math.sqrt(d_k)  # -> (Batch, h, seq_len, seq_len)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask==0,10**-9 ) # as -inf\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return (attention_scores @ value) , attention_scores  # attention value  = (Attention).V -> (batch, h, seq_len, d_v)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "\n",
        "    # Pass through the pre-attention projection: # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
        "    query = self.w_q(q)# Q'\n",
        "    key = self.w_k(k) # K'\n",
        "    value = self.w_v(v) # V'\n",
        "\n",
        "    # Separate different heads: # (batch, seq_len, d_model) -> (batch, seq_len, h, d_k)\n",
        "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k ) # view() reshapes the tensor without copying memory, similar to numpy's reshape().\n",
        "    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k )\n",
        "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_v )\n",
        "\n",
        "    # Transpose for attention dot product: batch x h x seq_len x d_v\n",
        "    query, key, value = query.transpose(1,2) , key.transpose(1,2), value.transpose(1,2)\n",
        "\n",
        "    # Created query, key and value for all heads of size d_k or d_v\n",
        "\n",
        "    # Attention Calculation\n",
        "    x, self.attention_scores = MultiHeadAttention.attention_calc(query, key, value, mask, self.dropout)\n",
        "\n",
        "    # right now we have h heads of matrices/ vectors of attention and we have to concatenate to get a output attention matrix\n",
        "    # (batch, h, seq_len, d_v) -> (batch, seq_len, h, d_v)\n",
        "    x = x.transpose(1,2)\n",
        "\n",
        "    # (batch, seq_len, h, d_v) -> (batch, seq_len, d_model)\n",
        "    x = x.contiguous().view(x.shape[0], -1, self.h*self.d_k)\n",
        "\n",
        "    return self.w_o(x) # batch, seq_len, d_model) -> batch, seq_len, d_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaUBbvhhECe5"
      },
      "source": [
        "## **Residual Connection**\n",
        "\n",
        "*   Skip connection used on layer connection in between Layer Normalization of sublayers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7JbP2LcmEFQb"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "  def __init__(self,dropout:float) -> None:\n",
        "\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm  = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL1UTTgIFbOS"
      },
      "source": [
        "# **ENCODER**\n",
        "\n",
        "Encoder has N number of following blocks:\n",
        "* Multi head Attention\n",
        "* Feed Forward\n",
        "* 2 Add & Norm\n",
        "\n",
        "Output of one block is fed as input to another and the output of last block is given to decoder.\n",
        "Initially, Input Embeddings and Positional Embeddings are provided to the first encoder block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rikYRho0FK3E"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardLayer, dropout:float) -> None:\n",
        "\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "# we don't want the padded space/words to interact with other words so we will be applying mask here\n",
        "  def forward(self, x, src_mask):\n",
        "\n",
        "    # one sent to normalization and one to multihead attention -> Norm. Added to send to feed forward, # feedforward is again normalized\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x,src_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Encoder has Nx EncoderBlocks\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, layers:nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETTLNxy9dCUs"
      },
      "source": [
        "# **Decoder**\n",
        "\n",
        "Has:\n",
        "* Masked Multihead attention\n",
        "* Multi Head attention block: Query from decoder and Key & value from encoder // CrossAttention\n",
        "* 3 Normalizations\n",
        "* Feed Forward Layer\n",
        "\n",
        "Output embeddings (shifted right) with positional embeddings are given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fsF6WIluWOah"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardLayer, dropout:float) -> None:\n",
        "\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "\n",
        "    self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "\n",
        "  def forward(self, x, encoder_op, src_mask, trg_mask): # src mask from encoder and trg=target mask for decoder\n",
        "    x = self.residual_connection[0](x, lambda x: self.self_attention_block(x,x,x,trg_mask))\n",
        "    x = self.residual_connection[1](x, lambda x: self.cross_attention_block(x,encoder_op, encoder_op, src_mask))\n",
        "    x = self.residual_connection[2](x, self.feed_forward_block)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, layers:nn.ModuleList):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, encoder_op, src_mask, trg_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_op, src_mask, trg_mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXNXz7YaUwPE"
      },
      "source": [
        "## Linear Projection Layer\n",
        "\n",
        "* The linear layer exists after Decoder.\n",
        "* Converts data from *`d_model`* into *`vocab_size`*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZzGFybRAUiUN"
      },
      "outputs": [],
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model:int, vocab_size:int):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "    # Using log_softmax instead of softmax for numerical stability\n",
        "    return torch.log_softmax(self.proj(x), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULBAJXfscVpC"
      },
      "source": [
        "# **TRANSFORMER**\n",
        "\n",
        "* **\n",
        "* **\n",
        "\n",
        "**Putting it all together.**\n",
        "\n",
        "* **\n",
        "* **\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvyBKq4pWUsN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K8xY74mkc1WP"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder:Encoder, decoder:Decoder, src_embed:InputEmbedding, trg_embed:InputEmbedding, src_pos:Positional_Embedding, trg_pos:Positional_Embedding, projection_layer:ProjectionLayer) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.src_pos = src_pos\n",
        "    self.trg_pos = trg_pos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src = self.src_embed(src)\n",
        "    src = self.src_pos(src)\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "\n",
        "\n",
        "  def decode(self, encoder_op, src_mask, trg, trg_mask):\n",
        "    trg = self.trg_embed(trg)\n",
        "    trg = self.trg_pos(trg)\n",
        "    return self.decoder(trg,encoder_op, src_mask, trg_mask)\n",
        "\n",
        "\n",
        "  def projection(self, x):\n",
        "    return self.projection_layer(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhHlO2tSWqWn"
      },
      "source": [
        "## Transformer Builder function\n",
        "\n",
        "Build transformer for translation task with hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tShudoHyWqBO"
      },
      "outputs": [],
      "source": [
        "def build_transformer(src_vocab_size:int, trg_vocab_size:int, src_seq_len:int, trg_seq_len:int, d_model:int=512, N:int=6, h:int=8, dropout:float=0.1, d_ff:int=2048) -> Transformer:\n",
        "  # Create the embedding layers\n",
        "    src_embed = InputEmbedding(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbedding(d_model, trg_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = Positional_Embedding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = Positional_Embedding(d_model, trg_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardLayer(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardLayer(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, trg_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOj197uhkDLD"
      },
      "source": [
        "### Transformer Model is obtained.\n",
        "\n",
        "This function returns a transformer model with the parameters provided and can be used for translation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer = build_transformer(\n",
        "    src_vocab_size=2000, # total tokens in train vocabulary\n",
        "    trg_vocab_size=1000, \n",
        "    src_seq_len=200, # max input length \n",
        "    trg_seq_len=100, # max output length\n",
        "    d_model=512, # dimension of model (embedding)\n",
        "    N=6, #Number of encoder/decoder blocks\n",
        "    h=8, # number of heads in Multi-head attention\n",
        "    dropout=0.1,\n",
        "    d_ff=2048 # dimension of feed forward layer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "igiNQPv-kTeF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x EncoderBlock(\n",
              "        (self_attention_block): MultiHeadAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardLayer(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-1): 2 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (norm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x DecoderBlock(\n",
              "        (self_attention_block): MultiHeadAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (cross_attention_block): MultiHeadAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardLayer(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual_connection): ModuleList(\n",
              "          (0-2): 3 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (norm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (src_embed): InputEmbedding(\n",
              "    (embedding): Embedding(2000, 512)\n",
              "  )\n",
              "  (trg_embed): InputEmbedding(\n",
              "    (embedding): Embedding(1000, 512)\n",
              "  )\n",
              "  (src_pos): Positional_Embedding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (trg_pos): Positional_Embedding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (projection_layer): ProjectionLayer(\n",
              "    (proj): Linear(in_features=512, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSVw9-_ckUAO"
      },
      "source": [
        "References:\n",
        "\n",
        "* [Yu-Hsiang Huang Github](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
        "* [Umar Jamil Github](https://github.com/hkproj/pytorch-transformer)\n",
        "\n",
        "With what I already knew about transformer, applying it in code helped me to understand it in more depth and also to clarify the doubts I had."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw5itB4_kZkc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
